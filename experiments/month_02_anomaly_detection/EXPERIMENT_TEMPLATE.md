# Experiment Log Template - PyTorch Anomaly Detection

> **Experiment Series**: Month 2 - Autoencoder-based Anomaly Detection
> **Model Type**: Variational Autoencoder (VAE) / Standard Autoencoder
> **Task**: Real-time anomaly detection on industrial sensor data
> **Success Criteria**: F1 Score > 0.90, Inference < 100ms p99, Precision > 0.85

---

## Experiment Metadata

| Field | Value |
|-------|-------|
| **Experiment ID** | `exp_001_baseline_autoencoder` |
| **Date Started** | YYYY-MM-DD |
| **Date Completed** | YYYY-MM-DD |
| **Researcher** | [Your Name] |
| **MLflow Run ID** | `<auto-generated>` |
| **Git Commit Hash** | `git rev-parse HEAD` |
| **Status** | üèóÔ∏è Running / ‚úÖ Completed / ‚ùå Failed / ‚è∏Ô∏è Paused |
| **Priority** | üî¥ High / üü° Medium / üü¢ Low |

---

## 1. Hypothesis & Objectives

### Research Question
> What question is this experiment trying to answer?

**Example**: Can a standard autoencoder detect anomalies in multivariate industrial sensor data with F1 > 0.90 using reconstruction error as the anomaly score?

### Hypothesis
> What do you expect to happen and why?

**Example**: A 3-layer autoencoder trained on normal sensor patterns will learn a compressed representation of healthy equipment behavior. During inference, anomalous patterns will produce higher reconstruction errors, enabling binary classification with a learned threshold at the 95th percentile of validation reconstruction errors.

### Primary Objective
- [ ] Achieve F1 score > 0.90 on held-out test set
- [ ] Maintain precision > 0.85 (minimize false alarms)
- [ ] Inference latency < 100ms at p99
- [ ] Model size < 50MB for edge deployment

### Secondary Objectives
- [ ] Interpretability: Identify which sensors contribute most to anomaly scores
- [ ] Robustness: Performance across all 4 anomaly types (SPIKE, DRIFT, CYCLIC, MULTI_SENSOR)
- [ ] Generalization: Test on unseen machine_id data

---

## 2. Dataset Description

### Data Source
```python
# InfluxDB query used
measurement = "sensor_readings"
time_range = "2026-01-15T00:00:00Z to 2026-01-25T23:59:59Z"
fields = ["temperature", "vibration", "pressure", "power_consumption"]
total_points = 1_728_000  # 10 days √ó 5 machines √ó 1 reading/sec √ó 4 sensors
```

### Data Splits

| Split | Time Range | Samples | Anomaly % | Purpose |
|-------|------------|---------|-----------|---------|
| **Train** | Jan 15-21 (7 days) | 1,209,600 | 5% | Model training (normal + some anomalies) |
| **Validation** | Jan 22-23 (2 days) | 345,600 | 5% | Hyperparameter tuning, threshold selection |
| **Test** | Jan 24-25 (2 days) | 172,800 | 5% | Final evaluation (unseen data) |

**Notes**:
- Train on 80% normal + 20% anomalous (to help model learn boundaries)
- Validation for early stopping and threshold calibration
- Test set includes unseen `machine_id` for generalization testing

### Feature Engineering

```python
# Features used
features = [
    "temperature",           # Raw sensor value (¬∞C)
    "vibration",            # Raw sensor value (mm/s)
    "pressure",             # Raw sensor value (PSI)
    "power_consumption",    # Raw sensor value (W)
    "temp_rolling_mean_10", # 10-second rolling mean
    "temp_rolling_std_10",  # 10-second rolling std
    "vibration_diff",       # First-order difference
    "hour_sin",             # Cyclical hour encoding (sin)
    "hour_cos",             # Cyclical hour encoding (cos)
]
```

**Normalization**: MinMaxScaler fit on training data, applied to all splits
**Window Size**: 60 time steps (1 minute of history)
**Stride**: 10 time steps (10-second overlap for real-time streaming)

### Data Quality Checks
- [x] No missing values (verified)
- [x] No future data leakage (time-based split)
- [x] Class balance documented (95% normal, 5% anomalous)
- [x] Outliers inspected (expected for anomalies)
- [x] Duplicate timestamps removed

---

## 3. Model Architecture

### Autoencoder Design

```python
# Model architecture
class SensorAutoencoder(nn.Module):
    def __init__(self, input_dim=9, latent_dim=4):
        super().__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, latent_dim),  # Bottleneck
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, input_dim),   # Reconstruct input
        )

    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed, latent
```

### Architecture Rationale
> Why this architecture?

- **Input dim = 9**: 4 raw sensors + 5 engineered features
- **Latent dim = 4**: Matches original sensor count for interpretability
- **Dropout = 0.2**: Regularization to prevent overfitting on normal patterns
- **ReLU activation**: Standard for autoencoders, fast convergence
- **No BatchNorm**: Simpler model, easier deployment

### Model Parameters
- **Total parameters**: ~10,000
- **Trainable parameters**: ~10,000
- **Model size**: ~40KB (well under 50MB target)
- **Complexity**: O(n) inference time

---

## 4. Training Configuration

### Hyperparameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **Batch size** | 64 | Balance GPU memory & gradient stability |
| **Learning rate** | 1e-3 | Adam default, good starting point |
| **LR Scheduler** | ReduceLROnPlateau | Adapt LR when validation loss plateaus |
| **Optimizer** | AdamW | Better generalization than Adam |
| **Weight decay** | 1e-5 | L2 regularization |
| **Max epochs** | 100 | Early stopping likely ~30-50 |
| **Early stopping** | Patience=10 | Stop if val_loss doesn't improve |
| **Loss function** | MSE (reconstruction) | Standard for autoencoders |
| **Gradient clipping** | Max norm=1.0 | Prevent exploding gradients |

### Training Hardware
- **Device**: CUDA / CPU / MPS (Apple Silicon)
- **GPU Model**: NVIDIA RTX 3080 (example)
- **Memory**: 16GB GPU RAM
- **Mixed Precision**: Enabled (float16) for faster training

### MLflow Logging
```python
# What to log
mlflow.log_params({
    "model_type": "autoencoder",
    "latent_dim": 4,
    "batch_size": 64,
    "learning_rate": 1e-3,
    "optimizer": "adamw",
})

mlflow.log_metrics({
    "train_loss": train_loss,
    "val_loss": val_loss,
    "f1_score": f1,
    "precision": precision,
    "recall": recall,
}, step=epoch)

mlflow.log_artifact("models/autoencoder_epoch_50.pth")
mlflow.log_artifact("plots/loss_curve.png")
mlflow.log_artifact("experiments/month_02/EXPERIMENT_LOG.md")
```

---

## 5. Evaluation Metrics

### Primary Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| **F1 Score** | > 0.90 | 0.XXX | ‚è≥ Pending |
| **Precision** | > 0.85 | 0.XXX | ‚è≥ Pending |
| **Recall** | > 0.80 | 0.XXX | ‚è≥ Pending |
| **Accuracy** | > 0.95 | 0.XXX | ‚è≥ Pending |
| **AUC-ROC** | > 0.95 | 0.XXX | ‚è≥ Pending |

### Confusion Matrix (Test Set)

|              | Predicted Normal | Predicted Anomaly |
|--------------|------------------|-------------------|
| **Actual Normal** | TN = XXX | FP = XXX |
| **Actual Anomaly** | FN = XXX | TP = XXX |

**Analysis**:
- False Positives (FP): [Describe patterns - which anomaly types are missed?]
- False Negatives (FN): [Describe false alarms - are they borderline cases?]

### Performance by Anomaly Type

| Anomaly Type | F1 Score | Precision | Recall | Notes |
|--------------|----------|-----------|--------|-------|
| **SPIKE** | 0.XXX | 0.XXX | 0.XXX | Sudden value jumps |
| **DRIFT** | 0.XXX | 0.XXX | 0.XXX | Gradual degradation |
| **CYCLIC** | 0.XXX | 0.XXX | 0.XXX | Repeating patterns |
| **MULTI_SENSOR** | 0.XXX | 0.XXX | 0.XXX | Correlated anomalies |

**Insights**:
- Which anomaly type is hardest to detect?
- Are MULTI_SENSOR anomalies easier (more signal)?

### Inference Performance

| Metric | Target | Actual | Notes |
|--------|--------|--------|-------|
| **Latency p50** | < 50ms | XX.Xms | Median inference time |
| **Latency p95** | < 80ms | XX.Xms | 95th percentile |
| **Latency p99** | < 100ms | XX.Xms | 99th percentile |
| **Throughput** | > 100 req/s | XXX req/s | Single GPU |
| **Memory usage** | < 2GB | X.XXG | GPU memory during inference |

**Profiling**:
```bash
# Command used for benchmarking
python scripts/benchmark_inference.py --model autoencoder_v1.pth --batch_size 1 --num_runs 1000
```

---

## 6. Results & Analysis

### Training Curves

**Loss Curve**:
![Training Loss](plots/loss_curve.png)

**Observations**:
- Training loss converged at epoch XX
- Validation loss started increasing at epoch XX (overfitting)
- Early stopping triggered at epoch XX
- Final train loss: X.XXXX, val loss: X.XXXX

**Reconstruction Error Distribution**:
![Reconstruction Error](plots/reconstruction_error_dist.png)

**Observations**:
- Normal samples: Mean reconstruction error = X.XXX, Std = X.XXX
- Anomalous samples: Mean reconstruction error = X.XXX, Std = X.XXX
- Clear separation between distributions (good sign)
- Threshold chosen at 95th percentile of validation normal errors: X.XXX

### Threshold Selection

| Threshold (Percentile) | Precision | Recall | F1 Score | False Alarm Rate |
|------------------------|-----------|--------|----------|------------------|
| 90th percentile | 0.XXX | 0.XXX | 0.XXX | XX% |
| 95th percentile | 0.XXX | 0.XXX | 0.XXX | XX% |
| 99th percentile | 0.XXX | 0.XXX | 0.XXX | XX% |

**Selected Threshold**: 95th percentile (balance precision/recall)

### Feature Importance

Which sensors contribute most to anomaly detection?

| Feature | Avg Reconstruction Error (Anomalies) | Contribution % |
|---------|--------------------------------------|----------------|
| Temperature | X.XXX | XX% |
| Vibration | X.XXX | XX% |
| Pressure | X.XXX | XX% |
| Power | X.XXX | XX% |

**Insights**:
- [e.g., "Vibration has highest reconstruction error during anomalies, suggesting it's the most informative feature"]

### Latent Space Visualization

**t-SNE Plot of Latent Representations**:
![Latent Space](plots/latent_space_tsne.png)

**Observations**:
- Normal samples cluster tightly (green)
- Anomalous samples spread out (red)
- Clear separation indicates good latent representation

---

## 7. Error Analysis

### False Positives (Type I Error)

**Example 1**:
- **Timestamp**: 2026-01-24 14:32:15
- **Machine ID**: machine_3
- **Predicted**: Anomaly (score: X.XXX)
- **Actual**: Normal
- **Analysis**: [e.g., "Legitimate shift change caused temp spike, but within normal bounds. Model threshold too sensitive."]

**Example 2**:
- Similar format...

**Pattern**: [Summarize common FP patterns]

### False Negatives (Type II Error)

**Example 1**:
- **Timestamp**: 2026-01-25 08:15:42
- **Machine ID**: machine_1
- **Predicted**: Normal (score: X.XXX)
- **Actual**: Anomaly (type: DRIFT)
- **Analysis**: [e.g., "Gradual drift was too subtle. Model trained mostly on spike anomalies. Need more drift examples in training."]

**Example 2**:
- Similar format...

**Pattern**: [Summarize common FN patterns]

### Recommendations for Next Iteration
1. [ ] Increase training data for DRIFT anomalies (currently underrepresented)
2. [ ] Experiment with VAE (Variational Autoencoder) for better uncertainty quantification
3. [ ] Add sensor-specific reconstruction losses (weighted)
4. [ ] Try LSTM autoencoder for temporal patterns
5. [ ] Ensemble with threshold-based rules for hybrid approach

---

## 8. Comparison with Baselines

| Model | F1 Score | Precision | Recall | Inference (ms) | Notes |
|-------|----------|-----------|--------|----------------|-------|
| **Rule-based threshold** | 0.75 | 0.68 | 0.85 | <1ms | Simple: `if temp > 85¬∞C` |
| **Isolation Forest** | 0.82 | 0.79 | 0.86 | ~20ms | scikit-learn baseline |
| **Standard Autoencoder (this)** | 0.XXX | 0.XXX | 0.XXX | ~XXms | Our experiment |
| **VAE (planned)** | - | - | - | - | Next experiment |
| **LSTM-AE (planned)** | - | - | - | - | Month 3 |

**Verdict**: [Did we beat baselines? By how much?]

---

## 9. Deployment Readiness

### Model Artifacts

```bash
experiments/month_02_anomaly_detection/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_final.pth          # Best checkpoint (20MB)
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_final.onnx         # ONNX export (18MB)
‚îÇ   ‚îî‚îÄ‚îÄ scaler.pkl                     # MinMaxScaler for preprocessing
‚îú‚îÄ‚îÄ metadata/
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml                    # Model hyperparameters
‚îÇ   ‚îú‚îÄ‚îÄ feature_names.json             # Input feature list
‚îÇ   ‚îî‚îÄ‚îÄ threshold.json                 # Anomaly threshold (0.XXX)
‚îî‚îÄ‚îÄ mlflow/
    ‚îî‚îÄ‚îÄ run_id.txt                     # MLflow run ID for traceability
```

### Deployment Checklist

- [x] Model exported to ONNX format
- [x] Preprocessing pipeline saved (scaler.pkl)
- [x] Threshold calibrated on validation set
- [ ] Integration test with Kafka stream (real-time)
- [ ] Load testing (1000 req/s sustained)
- [ ] Monitoring dashboard created (Grafana)
- [ ] Alerting rules configured (Prometheus)
- [ ] Model card documentation written
- [ ] API endpoint created (FastAPI `/predict`)
- [ ] Docker image built and pushed
- [ ] Kubernetes deployment manifest created

### API Contract

```python
# POST /predict
{
  "machine_id": "machine_1",
  "timestamp": "2026-01-25T10:30:00Z",
  "features": {
    "temperature": 72.5,
    "vibration": 1.8,
    "pressure": 55.2,
    "power_consumption": 285.0,
    # ... engineered features
  }
}

# Response
{
  "anomaly": true,
  "confidence": 0.87,
  "reconstruction_error": 0.0234,
  "threshold": 0.0210,
  "latency_ms": 45,
  "model_version": "v1.0.0_exp001"
}
```

---

## 10. Lessons Learned

### What Worked Well ‚úÖ
1. **Simple architecture**: Standard autoencoder was sufficient for first iteration
2. **Feature engineering**: Rolling statistics helped capture temporal patterns
3. **Data quality**: Clean InfluxDB data with no missing values
4. **[Add your own]**

### What Didn't Work ‚ùå
1. **Threshold sensitivity**: Fixed threshold struggles with drift anomalies
2. **Limited anomaly types in training**: Model biased toward spike detection
3. **No temporal modeling**: Single time-step input loses sequence context
4. **[Add your own]**

### Unexpected Findings üîç
1. **[e.g., "Multi-sensor anomalies are easiest to detect due to redundant signal"]**
2. **[e.g., "Night shift data has fewer anomalies, creating data imbalance"]**
3. **[Add your own]**

### Next Experiments
1. **Experiment 002**: Variational Autoencoder (VAE) for uncertainty quantification
2. **Experiment 003**: LSTM Autoencoder for temporal sequence modeling
3. **Experiment 004**: Ensemble (Autoencoder + Isolation Forest + Rules)
4. **Experiment 005**: Multi-task learning (anomaly + RUL prediction jointly)

---

## 11. Reproducibility

### Environment Setup

```bash
# Python environment
python --version  # 3.11.5

# Dependencies (exact versions)
pip install torch==2.0.1
pip install scikit-learn==1.3.0
pip install mlflow==2.8.1
pip install pandas==2.1.1
pip install numpy==1.24.3
pip install matplotlib==3.8.0
pip install seaborn==0.12.2
```

**Full requirements**: See `requirements.txt` in experiment folder

### Random Seeds

```python
# All random seeds fixed
RANDOM_SEED = 42

import random
import numpy as np
import torch

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
torch.cuda.manual_seed_all(RANDOM_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

### Data Version
- **DVC hash**: `abc123...` (if using DVC)
- **InfluxDB backup**: `s3://industrialmind-data/backups/2026-01-25.tar.gz`
- **Data collection period**: 2026-01-15 to 2026-01-25

### Training Script
```bash
# Command to reproduce exact results
python train_autoencoder.py \
  --config experiments/month_02/config.yaml \
  --data-start 2026-01-15 \
  --data-end 2026-01-25 \
  --seed 42 \
  --device cuda \
  --mlflow-experiment "month_02_anomaly_detection"
```

---

## 12. References & Resources

### Papers
1. **Original Autoencoder Paper**: [Citation]
2. **Anomaly Detection Survey**: [Citation]
3. **Industrial IoT Anomaly Detection**: [Citation]

### Code References
- Training script: `services/ml_platform/training/train_autoencoder.py`
- Model definition: `services/ml_platform/models/autoencoder.py`
- Data loader: `services/ml_platform/datasets/sensor_dataset.py`
- Evaluation: `services/ml_platform/evaluation/metrics.py`

### External Resources
- MLflow UI: http://localhost:5011
- Grafana dashboard: http://localhost:3011/d/ml-metrics
- Model card: `experiments/month_02/MODEL_CARD.md`

---

## 13. Approvals & Sign-off

| Role | Name | Date | Signature |
|------|------|------|-----------|
| **Researcher** | [Your Name] | YYYY-MM-DD | ‚úÖ |
| **Reviewer** | [Peer/Mentor] | YYYY-MM-DD | ‚è≥ |
| **Deployment Approval** | [Tech Lead] | YYYY-MM-DD | ‚è≥ |

---

## Appendix

### A. Hyperparameter Tuning Log

| Run | LR | Batch Size | Latent Dim | Dropout | Val Loss | F1 Score |
|-----|-----|------------|------------|---------|----------|----------|
| 1 | 1e-3 | 32 | 4 | 0.2 | 0.0123 | 0.XXX |
| 2 | 1e-3 | 64 | 4 | 0.2 | 0.0115 | 0.XXX |
| 3 | 1e-4 | 64 | 4 | 0.2 | 0.0135 | 0.XXX |
| 4 | 1e-3 | 64 | 8 | 0.2 | 0.0118 | 0.XXX |
| 5 | 1e-3 | 64 | 4 | 0.3 | 0.0120 | 0.XXX |

**Best Configuration**: Run #2

### B. Sample Predictions

```json
{
  "timestamp": "2026-01-25T10:30:00Z",
  "machine_id": "machine_1",
  "actual_label": "anomaly",
  "predicted_label": "anomaly",
  "confidence": 0.87,
  "reconstruction_error": 0.0234,
  "feature_contributions": {
    "temperature": 0.008,
    "vibration": 0.012,  // Highest contributor
    "pressure": 0.002,
    "power": 0.001
  }
}
```

### C. Visualizations

**Required Plots**:
1. ‚úÖ Training/Validation loss curve
2. ‚úÖ Reconstruction error distribution (normal vs anomaly)
3. ‚úÖ Confusion matrix heatmap
4. ‚úÖ ROC curve
5. ‚úÖ Precision-Recall curve
6. ‚úÖ t-SNE latent space visualization
7. ‚úÖ Per-sensor reconstruction error
8. ‚úÖ Threshold sensitivity analysis

**Plotting Script**: `scripts/plot_experiment_results.py`

---

**Experiment Status**: üèóÔ∏è In Progress
**Last Updated**: YYYY-MM-DD HH:MM
**Next Review**: YYYY-MM-DD

---

## Quick Links
- [Main Project README](../../README.md)
- [Architecture Documentation](../../docs/ARCHITECTURE.md)
- [MLflow Experiment](http://localhost:5011/#/experiments/1)
- [Training Scripts](../../services/ml_platform/training/)
- [Model Registry](../../models/)
